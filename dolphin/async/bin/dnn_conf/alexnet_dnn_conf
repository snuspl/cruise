# Copyright (C) 2016 Seoul National University
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# name: alexnet_dnn_conf
# description: neural network configuration for AlexNet database.
# Unlike AlexNet, this does not support grouping at convolutional layer and
# artificially enlarging the dataset using label-preserving transformation.
# See https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
# sections 3.5 and 4.1 respectively.

batch_size: 256
step_size: 0.01 #TODO #646: we need to decrease it every 100,000 iterations (about 20 epochs)
input_shape {
  dim: 3
  dim: 224
  dim: 224
}
#1
layer {
  type: "Convolutional"
  convolutional_param {
    kernel_height: 11
    kernel_width: 11
    stride_height: 4
    stride_width: 4
    init_weight: 0.01
    init_bias: 0
    num_output: 96
  }
}
layer {
  type: "Activation"
  activation_param {
    activation_function: "relu"
  }
}
layer {
  type: "LRN"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    k: 1
  }
}
layer {
  type: "Pooling"
  pooling_param {
    pooling_type: "MAX"
    kernel_height: 3
    kernel_width: 3
    stride_height: 2
    stride_width: 2
  }
}
#2
layer {
  type: "Convolutional"
  convolutional_param {
    kernel_height: 5
    kernel_width: 5
    padding_height: 2
    padding_width: 2
    init_weight: 0.01
    init_bias: 0.1
    num_output: 256
  }
}
layer {
  type: "Activation"
  activation_param {
    activation_function: "relu"
  }
}
layer {
  type: "LRN"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
    k: 1
  }
}
layer {
  type: "Pooling"
  pooling_param {
    pooling_type: "MAX"
    kernel_height: 3
    kernel_width: 3
    stride_height: 2
    stride_width: 2
  }
}
#3
layer {
  type: "Convolutional"
  convolutional_param {
    kernel_height: 3
    kernel_width: 3
    padding_height: 1
    padding_width: 1
    init_weight: 0.01
    init_bias: 0
    num_output: 384
  }
}
layer {
  type: "Activation"
  activation_param {
    activation_function: "relu"
  }
}
#4
layer {
  type: "Convolutional"
  convolutional_param {
    kernel_height: 3
    kernel_width: 3
    padding_height: 1
    padding_width: 1
    init_weight: 0.01
    init_bias: 0.1
    num_output: 384
  }
}
layer {
  type: "Activation"
  activation_param {
    activation_function: "relu"
  }
}
#5
layer {
  type: "Convolutional"
  convolutional_param {
    kernel_height: 3
    kernel_width: 3
    padding_height: 1
    padding_width: 1
    init_weight: 0.01
    init_bias: 0.1
    num_output: 256
  }
}
layer {
  type: "Activation"
  activation_param {
    activation_function: "relu"
  }
}
layer {
  type: "Pooling"
  pooling_param {
    pooling_type: "MAX"
    kernel_height: 3
    kernel_width: 3
    stride_height: 2
    stride_width: 2
  }
}
#6
layer {
  type: "FullyConnected"
  fully_connected_param {
    init_weight: 0.005
    init_bias: 0.1
    num_output: 4096
  }
}
layer {
  type: "Activation"
  activation_param {
    activation_function: "relu"
  }
}
layer {
  type: "Dropout"
  dropout_param {
    dropout_ratio: 0.5
  }
}
#7
layer {
  type: "FullyConnected"
  fully_connected_param {
    init_weight: 0.005
    init_bias: 0.1
    num_output: 4096
  }
}
layer {
  type: "Activation"
  activation_param {
    activation_function: "relu"
  }
}
layer {
  type: "Dropout"
  dropout_param {
    dropout_ratio: 0.5
  }
}
#8
layer {
  type: "FullyConnected"
  fully_connected_param {
    init_weight: 0.01
    init_bias: 0
    num_output: 1000
  }
}
layer {
  type: "ActivationWithLoss"
  activation_with_loss_param {
    activation_function: "softmax"
    loss_function: "crossEntropy"
  }
}
